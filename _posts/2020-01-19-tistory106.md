---
layout: post
title: PRML001 - Pattern Recognition 들어가기
categories: [scitech]
tags: [AI]
image:
  path:  /assets/img/posts/2020-01-19-PRML001/PRML001_02.png
  srcset:
    500w:  /assets/img/posts/2020-01-19-PRML001/PRML001_02_500.png
    1000w:  /assets/img/posts/2020-01-19-PRML001/PRML001_02_1000.png
    1500w:  /assets/img/posts/2020-01-19-PRML001/PRML001_02_1500.png
sitemap: false
hide_last_modified: true
hide_description: true
---

_A note on Pattern Recognition and Machine Learning by C. M. Bishop pp.1-4_



위 그림은 양자역학의 대부, 닐스보어가 제안한 양자가설을 설명해주는 그림이다. 고등학교 2학년 때 처음 배운 이 양자가설이 던져준 짜릿한 충격 덕분에, 한동안 보어와 아인슈타인 같은 물리학자들의 삶에 푹 빠져 살았다. 병은 점점 깊어지더니, 그로부터 수년간을 '나는 물리학자가 되겠다'는 신념으로 살았던 것 같다. 지금 돌이켜보면 참 귀여운 순간이었는데, 무엇 때문에 보어의 가설에 그리 반해버렸을까 하는 생각이 든다. 보어가 한 일은 사실 아주 간단한 일이었는데, 20세기 초에 물리학자들이 풀지 못하고 있던 **수소 원자 스펙트럼의 '패턴'을 '이해'해낸 것** 뿐이었다. 수소 원자를 뜨겁게 달구면 불연속적인 색깔의 빛들이 나오는데, 이는 당시의 물리학 이론으로는 설명할 수 없는 현상이었다. 그 때 사람들이 할 수 있는 것은 오로지 여러번의 실험을 통해 얻은 데이터를 쳐다보고 앉아있는 것 뿐이었는데, 어느날 몇몇 사람들이 빛의 파장 데이터에 특정한 패턴( $\\lambda^{-1} \\propto (\\frac{1}{n^2} -\\frac{1}{m^2})$ )이 존재한다는 사실을 알아챘다. 물리학자가 복잡한 논리를 펼쳐 얻어낸 결론이 아니라, 데이터 덕후였던 몇 사람들이 얻어낸 이 패턴들은, 지금 21세기를 빛내고 있는 수많은 과학기술의 초석이 되었다. 그리고, 내가 이 작업들을 보며 감동받았던 이유는, 누구의 도움이 없이도 '직관'에 의해 자연의 섭리를 이해했던 선배들 같이 나 또한 세상을 이해할 수 있는 눈을 갖게 될수있으리란 희망 때문이었던듯 하다. **자연현상을 유심히 지켜보고 얻어낸 통찰과 직관**만큼 귀한 자산이 또 있을까.

![](/assets/img/posts/2020-01-19-PRML001/PRML001_01.png)

비숍 선생이 쓴 이 책 **'Pattern recognition & Machine learning'(PRML)** 은 태양계의 궤도 패턴을 파악해 만유인력의 법칙의 발견에 기여한 티코 브라헤와, 양자역학의 초석이 된 원자 스펙트럼 이야기를 소개하며 시작한다. 인간이 자연을 배우는 방법에는 \*\*'인식과 학습'\*\*이라는 일련의 공통적인 과정이 있음을 강조하려는 포석으로 보인다. 책의 제목은 기계학습이란 단어가 주는 딱딱한 느낌을 주지만, PRML은 사실 **'우리 인간이 어떻게 인식하고 학습하는가?'** 라는 굉장히 매력적이고 어찌보면 감성적이기까지 한 주제를 다루는 학문인 것이다. PRML의 첫 페이지는 내 지난날 누렸던 물리학의 기쁨이 어떤 원리로 가능했는지 알려주겠다는, 굉장히 매력적인 teaser로 다가왔다.

그럼 이제 본론으로 들어가보자. '인식'이란 무엇인가? '학습'이란 무엇인가? 질문들은 어디서부터 매듭을 풀어야할지 모를정도로 큰 주제인만큼, 간단하면서도 PRML의 여러 중요한 요소를 담고 있는 **손글씨를 인식하는 문제**를 먼저 살펴보도록 하자.

![](/assets/img/posts/2020-01-19-PRML001/PRML001_03.png)

**MNIST dataset**: 손으로 쓴 숫자를 인식, 분류하는 문제다. 요즘 어도피 pdf 편집기에서 지원하는 문자인식기능(스캔한 사진 속에 있는 문자를 ASCII code로 바꾸는 기능)과 비슷한 문제로 생각하면 된다. MNIST Classifier(분류기)는 28x28(=784)픽셀로 이루어진 이미지를 0~9까지 10가지의 카테고리로 분류해하는 것을 목표로 한다. Classifier를 학습시킬 때는 각 입력 x 와 그에 해당하는 정답 y (MNIST의 경우엔 각 이미지의 아이덴티니인 0~9가운데 하나의 숫자)로 이루어진 training dataset을 활용해, x와 y의 관계를 설명하는 모델 f 을 찾아낸다. 학습이 완료된 classifier는 이미지를 분류하는데 쓰이는데, 임의의 이미지 x'를 학습된 모델의 입력으로 삼았을 때 그 결과값 y=f(x')이 분류된 결과를 알려준다. 결국, **학습과정은 학습데이터의 입력과 출력/정답/분류 속에 숨어있는 수학적 관계를 설명하는 모델 찾아내기의 과정**인 셈이고, 이를 활용하는 inference 혹은 test 과정은 학습 데이터에 없는 입력이더라도 학습데이터에 존재하는 수학적 관계를 전제해서 출력/정답/분류를 도출하는 과정을 말한다. 여기에서 짚고 넘어갈 몇가지 포인트들이 있는데...

-   **Adaptive learning** : 모델(예를 들어 유한개의 coefficient로 결정되는 polynominal)을 결정하는 parameter들은 수많은 training data에 의해 조금씩 영향을 받아 점진적으로 최적화 된다.
-   **Generalization** : training data를 통해 학습된 model은 test data를 처음 봄에도 불구하고, 올바른 출력을 내놔야만 한다. 이는 training data에 내제된 패턴과 그 패턴에 따른 출력/정답/분류를 결정하는 방법이 전혀 다른 dataset에서도 통하는 Generatlization('일반화')의 중요성을 알려주는 부분이다. 다시말해, 우리가 배우려는 것은 training dataset 자체가 아니라, 그 데이터가 대표하는 일반적 성질인 것이다.
-   **Preprocessing** : 사실 이미지 데이터는 MNIST dataset 처럼 항상 28x28 픽셀에 딱 맞게 만들어지지 않는다. 사람이 쓰는 글자의 크기, 카메라 해상도 설정 등등에 의해 입력의 포맷은 언제든 달라질 수 있기 때문에, 이의 크기를 조절하는 preprocessing (전처리) 과정을 통해 classifier를 더 넓은 범위의 데이터에 적용할 수 있다. 이에 더해, 입력 데이터의 차원수 (dimesionality)를 줄여 classifier의 computational cost를 줄이는 것도 preprocessing의 중요한 역할이다. 이를 feature extraction(특징추출, 패턴추출)이라고 하는데, 이 과정은 입력 데이터를 더 '잘 이해'하기 위해 필수적인 과정이다. 예를 들어, 얼굴 이미지를 분류하는 classifier가 있다고 했을 때, 눈/코/입 등 각 부위의 패턴을 분석해주는 preprocessor를 classifier 앞에 장착하면 그 분류 성능은 분명히 더 높아지게 될 것이다.

MNIST dataset에 대해 살펴보며 구체적인 task에 담긴 여러 요소들에 대해서 살펴보았다. 다음엔 시선을 나무 하나에서 숲으로 눈을 돌려서,machine learning의 종류와 분류법에 대해서 알아보도록 하자.

-   **Supervised learning(SL)** : 각 training 입력이 target값(혹은 label)을 갖고 있어서, model이 입력과 target값 사이의 관계를 설명하도록 만드는 학습방법. 앞서 설명한 MNIST를 예로 들 수 있다.
    -   **Classification**: 가능한 target값이 유한한 정수개로 한정할 수 있는 경우, 입력을 특정 카테고리로 분류하는 문제로 이해할수 있다. 사진을 보고 이것이 고양이인지, 개인지 맞추는 분류문제를 생각할 수 있다.
    -   **Regression**: target값이 연속적인 실수로만 표현할 수 있는 경우, 이는 산포된 데이터를 설명하는 특정 함수 곡선을 찾아내는 회귀문제(Regression)으로 이해할 수 있다.
-   **Unsupervised learning(USL)** : SL과 달리, target값과 무관하게 입력 데이터 자체에서 유의미한 정보를 얻어내고자 하는 학습방법이다.
    -   **Clustering** : 입력 데이터를 그룹지을 수 있는 표현 방법을 찾아내고, 그에 따라 입력 데이터들을 각 클러스터/그룹을 구분 짓는 것을 목표로 하는 task다.
    -   **Density Estimation** : 수많은 입력데이터를 보고, 입력 데이터의 확률분포를 찾아내는 것을 목표로 한다.
    -   **Visualization/Dimensionality Reduction** : 입력데이터를 더 낮은 차원으로 설명해 내는 것을 목표로 하며, 이는 데이터를 더 쉽게 표현할 수 있는 패터을 찾아내는 것으로 가능하기 때문에, feature extraction으로 불리기도 한다.
    -   **Generative model** : 앞선 방법들을 활용해 축약된 표현양식에서 원본의 상태를 복원하는 모델로, unsupervised learning에 속한다기보다는, 이를 이용하는 모델로 보면 좋을 것 같다.
-   **Reinforced learning(RL)** : 앞서 SL/USL과 달리, 수많은 training data가 쌓여있는 상태에서 학습을 진행하는 것이 아니라, trial and error 방식을 통해 데이터를 수집하고 학습하여 최대의 reward를 얻고자 하는 optimization 학습 기술이다.
    -   **Exploration vs Exploitation**: 데이터에 한정이 있는 만큼, 데이터를 수집하는 노력인 탐색단계(exploration)와 수집한 데이터를 기반으로 하여 특정 의사결정을 내리고 행동에 옮기는 수행단계(Exploitation)가 번갈아서 벌어진다. 데이터와 trial은 많을 수록 좋기 때문에, 둘 사이에 적절한 균형을 이루는 것이 매우 중요하다.

PRML에는 이렇게 다양한 학습 방법들이 있고, 그래서 공부할 것도 숨막힐 정도로 많다. 앞으로 책을 읽어 나가면서 차근차근 알게 되겠지만, 너무 방대한 학문을 손대는 것이 아닌가하는 걱정이 들기도 한다. 학생들의 이런 걱정을 미리 아셨는지 비숍선생님은, 우리의 첫 질문이었던 **'인식'이란 무엇인가?** 그리고 **인식을 '학습'하는 방법은 무엇인가?** 에 다시 초점을 맞추시고 **Bayesian paradigm**을 소개한다. 조건부 확률을 구하기 위한 계산법에 불과한 Bayesian rule이 어떻게 '인식'과 '학습'의 비밀을 알려주는지를 소개함으로서, 앞으로 PRML학생들이 앞서 설명한 수많은 알고리즘들을 소화해낼 수 있는 기초체력을 길러주신다.

그럼, Bayesian paradigm에 대한 이야기는 다음 포스팅에서 계속 !